{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.stats import norm\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"\u2705 Librer\u00edas importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# TEMA 6.III - NAIVE BAYES\n",
    "## Clasificaci\u00f3n Probabil\u00edstica Ingenua\n",
    "\n",
    "---\n",
    "## SECCI\u00d3N 1: Fundamentos de Probabilidad\n",
    "\n",
    "### 1.1 Concepto de Probabilidad Condicional\n",
    "\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
    "\n",
    "Probabilidad de A sabiendo que B ya ocurri\u00f3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Test de enfermedad\n",
    "P_enfermo = 0.01\n",
    "P_sano = 0.99\n",
    "P_test_pos_dado_enfermo = 0.99\n",
    "P_test_pos_dado_sano = 0.05\n",
    "\n",
    "P_test_pos = (P_test_pos_dado_enfermo * P_enfermo) + (P_test_pos_dado_sano * P_sano)\n",
    "P_enfermo_dado_test_pos = (P_test_pos_dado_enfermo * P_enfermo) / P_test_pos\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST DE ENFERMEDAD - TEOREMA DE BAYES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nP(Enfermo | Test+) = {P_enfermo_dado_test_pos:.1%}\")\n",
    "print(f\"\\nInterpretaci\u00f3n: Incluso dando positivo, solo hay {P_enfermo_dado_test_pos:.1%} probabilidad de estar enfermo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 El Teorema de Bayes\n",
    "\n",
    "$$P(\\text{Clase}|\\text{Datos}) = \\frac{P(\\text{Datos}|\\text{Clase}) \\cdot P(\\text{Clase})}{P(\\text{Datos})}$$\n",
    "\n",
    "En clasificaci\u00f3n:\n",
    "- **P(Clase | Datos)**: Probabilidad posterior (lo que queremos)\n",
    "- **P(Datos | Clase)**: Verosimilitud\n",
    "- **P(Clase)**: Prior\n",
    "- **P(Datos)**: Evidencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECCI\u00d3N 2: Naive Bayes - La Suposici\u00f3n Ingenua\n",
    "\n",
    "**Asume que todas las caracter\u00edsticas son independientes dado la clase.**\n",
    "\n",
    "\u00bfEs realista? \u274c NO\n",
    "\n",
    "\u00bfPor qu\u00e9 funciona? \u2705 \n",
    "- Simplifica c\u00e1lculos\n",
    "- Da buenos resultados\n",
    "- Es r\u00e1pido y necesita pocos datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECCI\u00d3N 3: Naive Bayes Gaussiano\n",
    "\n",
    "### 3.1 Distribuci\u00f3n Normal por clase\n",
    "\n",
    "Asumimos que cada caracter\u00edstica sigue una distribuci\u00f3n normal:\n",
    "\n",
    "$$P(x_i|\\text{Clase}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar Iris\n",
    "iris = load_iris()\n",
    "X_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y_iris = pd.Series(iris.target, name=\"species\")\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "feature_idx = 0\n",
    "feature_name = iris.feature_names[feature_idx]\n",
    "\n",
    "for class_idx in np.unique(y_iris):\n",
    "    data = X_iris[y_iris == class_idx][feature_name]\n",
    "    axes[0].hist(data, alpha=0.6, label=iris.target_names[class_idx], bins=15)\n",
    "\n",
    "axes[0].set_xlabel(feature_name)\n",
    "axes[0].set_ylabel('Frecuencia')\n",
    "axes[0].set_title('Distribuci\u00f3n por Clase')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "x_range = np.linspace(X_iris[feature_name].min(), X_iris[feature_name].max(), 200)\n",
    "for class_idx in np.unique(y_iris):\n",
    "    data = X_iris[y_iris == class_idx][feature_name]\n",
    "    mu = data.mean()\n",
    "    sigma = data.std()\n",
    "    y_dist = norm.pdf(x_range, mu, sigma)\n",
    "    axes[1].plot(x_range, y_dist, lw=2, label=iris.target_names[class_idx])\n",
    "    axes[1].fill_between(x_range, y_dist, alpha=0.2)\n",
    "\n",
    "axes[1].set_xlabel(feature_name)\n",
    "axes[1].set_ylabel('Densidad')\n",
    "axes[1].set_title('Distribuciones Normales')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "nb_gaussian = GaussianNB()\n",
    "nb_gaussian.fit(X_train, y_train)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PAR\u00c1METROS APRENDIDOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for class_idx, class_name in enumerate(iris.target_names):\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"  Prior: {nb_gaussian.class_prior_[class_idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Predicci\u00f3n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb_gaussian.predict(X_test)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUACI\u00d3N\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(\"\\nReporte:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=iris.target_names,\n",
    "            yticklabels=iris.target_names)\n",
    "plt.xlabel('Predicci\u00f3n')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusi\u00f3n')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECCI\u00d3N 4: Naive Bayes Multinomial (Textos)\n",
    "\n",
    "Para clasificar textos usando conteos de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"This movie is amazing and fantastic\",\n",
    "    \"I loved this film absolutely wonderful\",\n",
    "    \"Terrible movie waste of time\",\n",
    "    \"This is a bad and boring film\",\n",
    "    \"Great acting and excellent story\",\n",
    "    \"Horrible and disappointing\",\n",
    "    \"Best movie ever made\",\n",
    "    \"Worst film I have ever seen\"\n",
    "]\n",
    "\n",
    "labels = [1, 1, 0, 0, 1, 0, 1, 0]\n",
    "\n",
    "print(\"RESE\u00d1AS DE PEL\u00cdCULAS\")\n",
    "print(\"=\" * 60)\n",
    "for i, (review, label) in enumerate(zip(reviews, labels)):\n",
    "    sentiment = \"POS\" if label == 1 else \"NEG\"\n",
    "    print(f\"{i+1}. [{sentiment}] {review}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "X_text = vectorizer.fit_transform(reviews)\n",
    "\n",
    "print(\"\\nVocabulario:\", ', '.join(vectorizer.get_feature_names_out()))\n",
    "\n",
    "nb_multi = MultinomialNB()\n",
    "nb_multi.fit(X_text, labels)\n",
    "\n",
    "new_reviews = [\n",
    "    \"This movie is amazing\",\n",
    "    \"Terrible and boring film\"\n",
    "]\n",
    "\n",
    "X_new = vectorizer.transform(new_reviews)\n",
    "predictions = nb_multi.predict(X_new)\n",
    "probabilities = nb_multi.predict_proba(X_new)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREDICCIONES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for review, pred, probs in zip(new_reviews, predictions, probabilities):\n",
    "    sentiment = \"POSITIVO\" if pred == 1 else \"NEGATIVO\"\n",
    "    print(f\"\\n{review}\")\n",
    "    print(f\"  Predicci\u00f3n: {sentiment}\")\n",
    "    print(f\"  Confianza: {max(probs):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECCI\u00d3N 5: Detecci\u00f3n de Spam\n",
    "\n",
    "Caso real: Clasificar emails como spam o leg\u00edtimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_words = [\n",
    "    \"Click here now\",\n",
    "    \"Limited time offer\",\n",
    "    \"Free money today\",\n",
    "    \"Act now immediately\",\n",
    "    \"Call now\"\n",
    "]\n",
    "\n",
    "ham_words = [\n",
    "    \"Hi how are you\",\n",
    "    \"Meeting tomorrow\",\n",
    "    \"Thanks for your help\",\n",
    "    \"See you soon\",\n",
    "    \"Have a great day\"\n",
    "]\n",
    "\n",
    "all_texts = spam_words + ham_words\n",
    "all_labels = [1]*len(spam_words) + [0]*len(ham_words)\n",
    "\n",
    "vectorizer_spam = CountVectorizer(lowercase=True, stop_words='english')\n",
    "X_spam = vectorizer_spam.fit_transform(all_texts)\n",
    "\n",
    "nb_spam = MultinomialNB()\n",
    "nb_spam.fit(X_spam, all_labels)\n",
    "\n",
    "print(f\"Accuracy en training: {nb_spam.score(X_spam, all_labels):.1%}\")\n",
    "\n",
    "new_emails = [\n",
    "    \"Click here for amazing offer\",\n",
    "    \"Hi let's meet tomorrow\"\n",
    "]\n",
    "\n",
    "X_new_emails = vectorizer_spam.transform(new_emails)\n",
    "spam_pred = nb_spam.predict(X_new_emails)\n",
    "spam_proba = nb_spam.predict_proba(X_new_emails)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREDICCIONES DE SPAM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for email, pred, proba in zip(new_emails, spam_pred, spam_proba):\n",
    "    status = \"\u26a0\ufe0f SPAM\" if pred == 1 else \"\u2705 LEG\u00cdTIMO\"\n",
    "    print(f\"\\n{email}\")\n",
    "    print(f\"  {status}\")\n",
    "    print(f\"  Confianza: {max(proba):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CONCLUSIONES\n",
    "\n",
    "\u2705 **Ventajas:**\n",
    "- R\u00e1pido de entrenar\n",
    "- Funciona con pocos datos\n",
    "- Interpretable\n",
    "- Excelente para textos\n",
    "\n",
    "\u274c **Desventajas:**\n",
    "- Asunci\u00f3n de independencia no siempre v\u00e1lida\n",
    "- No captura relaciones complejas\n",
    "\n",
    "\ud83d\udcca **Casos de uso:**\n",
    "- Clasificaci\u00f3n de textos\n",
    "- Detecci\u00f3n de spam\n",
    "- An\u00e1lisis de sentimientos\n",
    "- Filtros de correo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}