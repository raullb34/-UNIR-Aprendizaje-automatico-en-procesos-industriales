{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74411902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8b4906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.stats import norm\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a29fe83",
   "metadata": {},
   "source": [
    "---\n",
    "## SECCI√ìN 1: Fundamentos de Probabilidad\n",
    "\n",
    "### 1.1 Conceptos b√°sicos\n",
    "\n",
    "**Probabilidad**: Medida de qu√© tan probable es que ocurra un evento.\n",
    "\n",
    "$$P(A) = \\frac{\\text{Casos favorables}}{\\text{Casos totales}}$$\n",
    "\n",
    "**Ejemplo:** Probabilidad de sacar un as en una baraja de 52 cartas:\n",
    "$$P(\\text{As}) = \\frac{4}{52} \\approx 0.077$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ce69e2",
   "metadata": {},
   "source": [
    "### 1.2 Probabilidad Condicional\n",
    "\n",
    "**Probabilidad condicional**: Probabilidad de A sabiendo que B ya ocurri√≥.\n",
    "\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
    "\n",
    "**Ejemplo en medicina:** Si haces un test y da positivo, ¬øcu√°l es la probabilidad real de estar enfermo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6b4e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Test de enfermedad\n",
    "P_enfermo = 0.01\n",
    "P_sano = 0.99\n",
    "P_test_pos_dado_enfermo = 0.99\n",
    "P_test_pos_dado_sano = 0.05\n",
    "\n",
    "P_test_pos = (P_test_pos_dado_enfermo * P_enfermo) + (P_test_pos_dado_sano * P_sano)\n",
    "P_enfermo_dado_test_pos = (P_test_pos_dado_enfermo * P_enfermo) / P_test_pos\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EJEMPLO: TEST DE ENFERMEDAD\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìã Datos iniciales:\")\n",
    "print(f\"  P(Enfermo) = {P_enfermo:.1%}\")\n",
    "print(f\"  P(Test+ | Enfermo) = {P_test_pos_dado_enfermo:.1%}  (sensibilidad)\")\n",
    "print(f\"  P(Test+ | Sano) = {P_test_pos_dado_sano:.1%}       (falso positivo)\")\n",
    "print(f\"\\nüî¢ C√°lculos:\")\n",
    "print(f\"  P(Test+) = {P_test_pos:.4f}\")\n",
    "print(f\"  P(Enfermo | Test+) = {P_enfermo_dado_test_pos:.1%}\")\n",
    "print(f\"\\nüí° Interpretaci√≥n:\")\n",
    "print(f\"  Incluso si das positivo, solo hay {P_enfermo_dado_test_pos:.1%} de probabilidad de estar enfermo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ac69ee",
   "metadata": {},
   "source": [
    "### 1.3 El Teorema de Bayes\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\n",
    "\n",
    "**En clasificaci√≥n:** $P(\\text{Clase}|\\text{Datos}) = \\frac{P(\\text{Datos}|\\text{Clase}) \\cdot P(\\text{Clase})}{P(\\text{Datos})}$\n",
    "\n",
    "- **P(Clase | Datos)**: Probabilidad posterior (lo que queremos)\n",
    "- **P(Datos | Clase)**: Verosimilitud\n",
    "- **P(Clase)**: Prior\n",
    "- **P(Datos)**: Evidencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81701f7",
   "metadata": {},
   "source": [
    "---\n",
    "## SECCI√ìN 2: Naive Bayes - La Suposici√≥n Ingenua\n",
    "\n",
    "### 2.1 ¬øPor qu√© \"Naive\"?\n",
    "\n",
    "Naive Bayes **asume que todas las caracter√≠sticas son independientes** dado la clase.\n",
    "\n",
    "**¬øEs realista?** ‚ùå NO, pero funciona bien en pr√°ctica porque:\n",
    "- Simplifica enormemente los c√°lculos\n",
    "- A menudo da buenos resultados\n",
    "- Es especialmente bueno en an√°lisis de textos\n",
    "- Es **r√°pido** y necesita **pocos datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1e50ec",
   "metadata": {},
   "source": [
    "---\n",
    "## SECCI√ìN 3: Naive Bayes Gaussiano (Caracter√≠sticas Continuas)\n",
    "\n",
    "### 3.1 Distribuci√≥n Normal por clase\n",
    "\n",
    "Asumimos que cada caracter√≠stica sigue una distribuci√≥n normal dentro de cada clase:\n",
    "\n",
    "$$P(x_i|\\text{Clase}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf9fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset Iris\n",
    "iris = load_iris()\n",
    "X_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y_iris = pd.Series(iris.target, name=\"species\")\n",
    "\n",
    "# Visualizar distribuci√≥n\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "feature_idx = 0\n",
    "feature_name = iris.feature_names[feature_idx]\n",
    "\n",
    "# Histograma\n",
    "for class_idx in np.unique(y_iris):\n",
    "    data = X_iris[y_iris == class_idx][feature_name]\n",
    "    axes[0].hist(data, alpha=0.6, label=iris.target_names[class_idx], bins=15)\n",
    "\n",
    "axes[0].set_xlabel(feature_name)\n",
    "axes[0].set_ylabel('Frecuencia')\n",
    "axes[0].set_title('Distribuci√≥n por Clase')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Curvas normales\n",
    "x_range = np.linspace(X_iris[feature_name].min(), X_iris[feature_name].max(), 200)\n",
    "for class_idx in np.unique(y_iris):\n",
    "    data = X_iris[y_iris == class_idx][feature_name]\n",
    "    mu = data.mean()\n",
    "    sigma = data.std()\n",
    "    y_dist = norm.pdf(x_range, mu, sigma)\n",
    "    axes[1].plot(x_range, y_dist, lw=2, label=iris.target_names[class_idx])\n",
    "    axes[1].fill_between(x_range, y_dist, alpha=0.2)\n",
    "\n",
    "axes[1].set_xlabel(feature_name)\n",
    "axes[1].set_ylabel('Densidad de Probabilidad')\n",
    "axes[1].set_title('Distribuciones Normales (Asumidas)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Cada clase tiene su propia distribuci√≥n normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc6af81",
   "metadata": {},
   "source": [
    "### 3.2 Entrenamiento\n",
    "\n",
    "El entrenamiento solo calcula Œº y œÉ para cada caracter√≠stica en cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988a46e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "# Entrenar modelo\n",
    "nb_gaussian = GaussianNB()\n",
    "nb_gaussian.fit(X_train, y_train)\n",
    "\n",
    "# Ver par√°metros\n",
    "print(\"=\" * 60)\n",
    "print(\"PAR√ÅMETROS APRENDIDOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for class_idx, class_name in enumerate(iris.target_names):\n",
    "    print(f\"\\nüîπ Clase: {class_name}\")\n",
    "    print(f\"   Medias (Œº):\")\n",
    "    for feat_idx, feat_name in enumerate(iris.feature_names):\n",
    "        mu = nb_gaussian.theta_[class_idx, feat_idx]\n",
    "        print(f\"     {feat_name}: {mu:.3f}\")\n",
    "    \n",
    "    print(f\"   Desviaciones (œÉ):\")\n",
    "    for feat_idx, feat_name in enumerate(iris.feature_names):\n",
    "        sigma = np.sqrt(nb_gaussian.var_[class_idx, feat_idx])\n",
    "        print(f\"     {feat_name}: {sigma:.3f}\")\n",
    "    \n",
    "    print(f\"   Prior P(Clase): {nb_gaussian.class_prior_[class_idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90c280a",
   "metadata": {},
   "source": [
    "### 3.3 Predicci√≥n manual paso a paso\n",
    "\n",
    "Vamos a predecir la clase de un ejemplo calculando las probabilidades manualmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a5ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomar un ejemplo\n",
    "sample_idx = 5\n",
    "sample = X_test.iloc[sample_idx].values\n",
    "true_class = y_test.iloc[sample_idx]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREDICCI√ìN MANUAL CON TEOREMA DE BAYES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìä Flor a clasificar:\")\n",
    "for i, feat_name in enumerate(iris.feature_names):\n",
    "    print(f\"   {feat_name}: {sample[i]:.2f}\")\n",
    "print(f\"\\nClase real: {iris.target_names[true_class]}\")\n",
    "\n",
    "# Funci√≥n para calcular P(x|clase)\n",
    "def gaussian_prob(x, mu, sigma):\n",
    "    numerator = np.exp(-((x - mu)**2) / (2 * sigma**2))\n",
    "    denominator = np.sqrt(2 * np.pi * sigma**2)\n",
    "    return numerator / denominator\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"C√ÅLCULO DETALLADO POR CADA CLASE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "posteriors = []\n",
    "for class_idx in np.unique(y_train):\n",
    "    prior = np.log(nb_gaussian.class_prior_[class_idx])\n",
    "    likelihood = 0\n",
    "    \n",
    "    print(f\"\\nüî∏ Clase: {iris.target_names[class_idx]}\")\n",
    "    print(f\"   Prior: P(Clase) = {nb_gaussian.class_prior_[class_idx]:.4f}\")\n",
    "    print(f\"   Likelihoods (caracter√≠sticas):\")\n",
    "    \n",
    "    for feat_idx in range(len(sample)):\n",
    "        mu = nb_gaussian.theta_[class_idx, feat_idx]\n",
    "        sigma = np.sqrt(nb_gaussian.var_[class_idx, feat_idx])\n",
    "        x_val = sample[feat_idx]\n",
    "        \n",
    "        prob = gaussian_prob(x_val, mu, sigma)\n",
    "        likelihood += np.log(prob)\n",
    "        print(f\"     {iris.feature_names[feat_idx]}: P({x_val:.2f}|clase) = {prob:.6f}\")\n",
    "    \n",
    "    posterior = prior + likelihood\n",
    "    posteriors.append(posterior)\n",
    "    print(f\"   Posterior total (log): {posterior:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTADO\")\n",
    "print(\"=\" * 70)\n",
    "best_class_idx = np.argmax(posteriors)\n",
    "print(f\"\\n‚úÖ PREDICCI√ìN: {iris.target_names[best_class_idx]}\")\n",
    "print(f\"‚úì Correcta: {'S√ç ‚úÖ' if best_class_idx == true_class else 'NO ‚ùå'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8cb9af",
   "metadata": {},
   "source": [
    "### 3.4 Probabilidades predichas\n",
    "\n",
    "El modelo tambi√©n devuelve probabilidades (no solo la clase predicha):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13b0ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener probabilidades\n",
    "y_pred_proba = nb_gaussian.predict_proba(X_test)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROBABILIDADES PREDICHAS (PRIMEROS 5 EJEMPLOS)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\nüìå Ejemplo {i+1}:\")\n",
    "    print(f\"   Real: {iris.target_names[y_test.iloc[i]]}\")\n",
    "    for class_idx, class_name in enumerate(iris.target_names):\n",
    "        prob = y_pred_proba[i, class_idx]\n",
    "        bar = \"‚ñà\" * int(prob * 20)\n",
    "        print(f\"   {class_name:15s}: {prob:.4f} {bar}\")\n",
    "    pred = nb_gaussian.predict([X_test.iloc[i].values])[0]\n",
    "    print(f\"   ‚ûú Predicci√≥n: {iris.target_names[pred]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341f4077",
   "metadata": {},
   "source": [
    "### 3.5 Evaluaci√≥n del modelo\n",
    "\n",
    "M√©tricas est√°ndar de clasificaci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones\n",
    "y_pred = nb_gaussian.predict(X_test)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUACI√ìN EN TEST SET\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(\"\\nReporte de Clasificaci√≥n:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=iris.target_names,\n",
    "            yticklabels=iris.target_names)\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusi√≥n')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c488cec3",
   "metadata": {},
   "source": [
    "---\n",
    "## SECCI√ìN 4: Naive Bayes Multinomial (Para Textos)\n",
    "\n",
    "### 4.1 Problema: Clasificaci√≥n de rese√±as\n",
    "\n",
    "Queremos clasificar rese√±as de pel√≠culas como positivas o negativas bas√°ndonos en las palabras que contienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c9d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Rese√±as de pel√≠culas\n",
    "reviews = [\n",
    "    \"This movie is amazing and fantastic\",\n",
    "    \"I loved this film absolutely wonderful\",\n",
    "    \"Terrible movie waste of time\",\n",
    "    \"This is a bad and boring film\",\n",
    "    \"Great acting and excellent story\",\n",
    "    \"Horrible and disappointing\",\n",
    "    \"Best movie ever made\",\n",
    "    \"Worst film I have ever seen\"\n",
    "]\n",
    "\n",
    "labels = [1, 1, 0, 0, 1, 0, 1, 0]  # 1=Positivo, 0=Negativo\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATASET: RESE√ëAS DE PEL√çCULAS\")\n",
    "print(\"=\" * 70)\n",
    "for i, (review, label) in enumerate(zip(reviews, labels)):\n",
    "    sentiment = \"POSITIVO üü¢\" if label == 1 else \"NEGATIVO üî¥\"\n",
    "    print(f\"{i+1}. [{sentiment}] {review}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d78bba",
   "metadata": {},
   "source": [
    "### 4.2 Vectorizaci√≥n: Bag of Words\n",
    "\n",
    "Convertimos cada texto en un vector de conteos de palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f0602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir textos a matriz de conteos\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "X_text = vectorizer.fit_transform(reviews)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"=\" * 70)\n",
    "print(\"VOCABULARIO APRENDIDO\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Palabras √∫nicas: {len(feature_names)}\")\n",
    "print(f\"Palabras: {', '.join(feature_names)}\")\n",
    "\n",
    "# Mostrar matriz de conteos\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MATRIZ DE CONTEOS (BAG OF WORDS)\")\n",
    "print(\"=\" * 70)\n",
    "df_counts = pd.DataFrame(\n",
    "    X_text.toarray(),\n",
    "    columns=feature_names\n",
    ")\n",
    "df_counts['Sentimiento'] = ['POSITIVO' if l == 1 else 'NEGATIVO' for l in labels]\n",
    "print(df_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22c6df8",
   "metadata": {},
   "source": [
    "### 4.3 Entrenamiento Multinomial\n",
    "\n",
    "Naive Bayes Multinomial trabaja directamente con conteos de palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar\n",
    "nb_multi = MultinomialNB()\n",
    "nb_multi.fit(X_text, labels)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PAR√ÅMETROS APRENDIDOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüî¥ CLASE NEGATIVA (0):\")\n",
    "for word, prob in zip(feature_names, nb_multi.feature_log_prob_[0]):\n",
    "    print(f\"     {word:15s}: {np.exp(prob):.4f}\")\n",
    "\n",
    "print(\"\\nüü¢ CLASE POSITIVA (1):\")\n",
    "for word, prob in zip(feature_names, nb_multi.feature_log_prob_[1]):\n",
    "    print(f\"     {word:15s}: {np.exp(prob):.4f}\")\n",
    "\n",
    "print(f\"\\nüìä Probabilidades iniciales (priors):\")\n",
    "print(f\"   P(NEGATIVO) = {np.exp(nb_multi.class_log_prior_[0]):.4f}\")\n",
    "print(f\"   P(POSITIVO) = {np.exp(nb_multi.class_log_prior_[1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8be523",
   "metadata": {},
   "source": [
    "### 4.4 Predicci√≥n en textos nuevos\n",
    "\n",
    "Clasificamos textos que el modelo nunca ha visto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8549cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textos nuevos\n",
    "new_reviews = [\n",
    "    \"This movie is amazing\",\n",
    "    \"Terrible and boring film\"\n",
    "]\n",
    "\n",
    "X_new = vectorizer.transform(new_reviews)\n",
    "predictions = nb_multi.predict(X_new)\n",
    "probabilities = nb_multi.predict_proba(X_new)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREDICCIONES EN TEXTOS NUEVOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for review, pred, probs in zip(new_reviews, predictions, probabilities):\n",
    "    sentiment = \"POSITIVO üü¢\" if pred == 1 else \"NEGATIVO üî¥\"\n",
    "    print(f\"\\nüìù Texto: \\\"{review}\\\"\")\n",
    "    print(f\"   Predicci√≥n: {sentiment}\")\n",
    "    print(f\"   P(NEGATIVO) = {probs[0]:.4f}\")\n",
    "    print(f\"   P(POSITIVO) = {probs[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718da524",
   "metadata": {},
   "source": [
    "### 4.5 Palabras m√°s informativas\n",
    "\n",
    "¬øQu√© palabras son m√°s importantes para cada sentimiento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b6fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diferencia entre clases\n",
    "logprob_pos = nb_multi.feature_log_prob_[1]\n",
    "logprob_neg = nb_multi.feature_log_prob_[0]\n",
    "diff = logprob_pos - logprob_neg\n",
    "\n",
    "# Top 5\n",
    "top_pos_idx = np.argsort(diff)[-5:]\n",
    "top_neg_idx = np.argsort(diff)[:5]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PALABRAS M√ÅS INFORMATIVAS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüü¢ PALABRAS ASOCIADAS A POSITIVO:\")\n",
    "for idx in reversed(top_pos_idx):\n",
    "    word = feature_names[idx]\n",
    "    print(f\"   {word}\")\n",
    "\n",
    "print(\"\\nüî¥ PALABRAS ASOCIADAS A NEGATIVO:\")\n",
    "for idx in top_neg_idx:\n",
    "    word = feature_names[idx]\n",
    "    print(f\"   {word}\")\n",
    "\n",
    "# Visualizar\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "words_pos = feature_names[top_pos_idx]\n",
    "diffs_pos = diff[top_pos_idx]\n",
    "ax1.barh(words_pos, diffs_pos, color='green', alpha=0.7)\n",
    "ax1.set_xlabel('Diferencia de Log-Probabilidad')\n",
    "ax1.set_title('Palabras Indicadoras de Rese√±a POSITIVA')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "words_neg = feature_names[top_neg_idx]\n",
    "diffs_neg = diff[top_neg_idx]\n",
    "ax2.barh(words_neg, diffs_neg, color='red', alpha=0.7)\n",
    "ax2.set_xlabel('Diferencia de Log-Probabilidad')\n",
    "ax2.set_title('Palabras Indicadoras de Rese√±a NEGATIVA')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8066e41",
   "metadata": {},
   "source": [
    "---\n",
    "## SECCI√ìN 5: Caso Real - Detecci√≥n de Spam\n",
    "\n",
    "Clasificar emails como spam o leg√≠timos usando palabras clave:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab330c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "spam_words = [\n",
    "    \"Click here now\",\n",
    "    \"Limited time offer\",\n",
    "    \"Free money today\",\n",
    "    \"Act now immediately\",\n",
    "    \"Call now\"\n",
    "]\n",
    "\n",
    "ham_words = [\n",
    "    \"Hi how are you\",\n",
    "    \"Meeting tomorrow\",\n",
    "    \"Thanks for your help\",\n",
    "    \"See you soon\",\n",
    "    \"Have a great day\"\n",
    "]\n",
    "\n",
    "all_texts = spam_words + ham_words\n",
    "all_labels = [1]*len(spam_words) + [0]*len(ham_words)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATASET: SPAM vs EMAILS LEG√çTIMOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüî¥ SPAM:\")\n",
    "for text in spam_words:\n",
    "    print(f\"   {text}\")\n",
    "\n",
    "print(\"\\nüü¢ LEG√çTIMO (HAM):\")\n",
    "for text in ham_words:\n",
    "    print(f\"   {text}\")\n",
    "\n",
    "# Vectorizar\n",
    "vectorizer_spam = CountVectorizer(lowercase=True, stop_words='english')\n",
    "X_spam = vectorizer_spam.fit_transform(all_texts)\n",
    "\n",
    "# Entrenar\n",
    "nb_spam = MultinomialNB()\n",
    "nb_spam.fit(X_spam, all_labels)\n",
    "\n",
    "print(f\"\\n‚úÖ Accuracy en training: {nb_spam.score(X_spam, all_labels):.1%}\")\n",
    "\n",
    "# Predecir en textos nuevos\n",
    "new_emails = [\n",
    "    \"Click here for amazing offer\",\n",
    "    \"Hi let's meet tomorrow\"\n",
    "]\n",
    "\n",
    "X_new_emails = vectorizer_spam.transform(new_emails)\n",
    "spam_pred = nb_spam.predict(X_new_emails)\n",
    "spam_proba = nb_spam.predict_proba(X_new_emails)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PREDICCIONES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for email, pred, proba in zip(new_emails, spam_pred, spam_proba):\n",
    "    status = \"‚ö†Ô∏è SPAM\" if pred == 1 else \"‚úÖ LEG√çTIMO\"\n",
    "    print(f\"\\nüìß {email}\")\n",
    "    print(f\"   Clasificaci√≥n: {status}\")\n",
    "    print(f\"   Confianza: {max(proba):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5cfbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.stats import norm\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aa73d3",
   "metadata": {},
   "source": [
    "### 1.2 Probabilidad Condicional\n",
    "\n",
    "**Probabilidad condicional**: Probabilidad de A sabiendo que B ya ocurri√≥.\n",
    "\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
    "\n",
    "**Ejemplo en medicina:**\n",
    "Si haces un test y da positivo, ¬øcu√°l es la probabilidad real de estar enfermo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f2f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Test de enfermedad\n",
    "# P(Enfermo) = 0.01 (1% de poblaci√≥n est√° enferma)\n",
    "# P(Test+ | Enfermo) = 0.99 (99% de sensibilidad)\n",
    "# P(Test+ | Sano) = 0.05 (5% de falsos positivos)\n",
    "\n",
    "P_enfermo = 0.01\n",
    "P_sano = 0.99\n",
    "P_test_pos_dado_enfermo = 0.99\n",
    "P_test_pos_dado_sano = 0.05\n",
    "\n",
    "# Probabilidad total de dar test positivo\n",
    "P_test_pos = (P_test_pos_dado_enfermo * P_enfermo) + (P_test_pos_dado_sano * P_sano)\n",
    "\n",
    "# TEOREMA DE BAYES: P(Enfermo | Test+)\n",
    "P_enfermo_dado_test_pos = (P_test_pos_dado_enfermo * P_enfermo) / P_test_pos\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EJEMPLO: TEST DE ENFERMEDAD\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìã Datos iniciales:\")\n",
    "print(f\"  P(Enfermo) = {P_enfermo:.1%}\")\n",
    "print(f\"  P(Test+ | Enfermo) = {P_test_pos_dado_enfermo:.1%}  (sensibilidad)\")\n",
    "print(f\"  P(Test+ | Sano) = {P_test_pos_dado_sano:.1%}       (falso positivo)\")\n",
    "\n",
    "print(f\"\\nüî¢ C√°lculos:\")\n",
    "print(f\"  P(Test+) = {P_test_pos:.4f}\")\n",
    "print(f\"  P(Enfermo | Test+) = {P_enfermo_dado_test_pos:.4f} = {P_enfermo_dado_test_pos:.1%}\")\n",
    "\n",
    "print(f\"\\nüí° Interpretaci√≥n:\")\n",
    "print(f\"  Incluso si das positivo, solo hay {P_enfermo_dado_test_pos:.1%} de probabilidad de estar enfermo\")\n",
    "print(f\"  ¬øPor qu√©? Porque la enfermedad es rara (1% en poblaci√≥n)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056f656a",
   "metadata": {},
   "source": [
    "### 1.3 El Teorema de Bayes\n",
    "\n",
    "El Teorema de Bayes nos permite calcular probabilidades \"hacia atr√°s\":\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\n",
    "\n",
    "**En contexto de clasificaci√≥n:**\n",
    "$$P(\\text{Clase}|\\text{Datos}) = \\frac{P(\\text{Datos}|\\text{Clase}) \\cdot P(\\text{Clase})}{P(\\text{Datos})}$$\n",
    "\n",
    "**Significado:**\n",
    "- **P(Clase | Datos)**: Probabilidad posterior (lo que queremos saber)\n",
    "- **P(Datos | Clase)**: Verosimilitud (likelihood)\n",
    "- **P(Clase)**: Probabilidad previa (prior)\n",
    "- **P(Datos)**: Evidencia (normalizador)\n",
    "\n",
    "---\n",
    "## SECCI√ìN 2: Naive Bayes - La Suposici√≥n Ingenua\n",
    "\n",
    "### 2.1 ¬øPor qu√© \"Naive\"?\n",
    "\n",
    "Naive Bayes **asume que todas las caracter√≠sticas son independientes** dado la clase.\n",
    "\n",
    "**¬øEs realista?** ‚ùå NO, pero funciona bien en pr√°ctica porque:\n",
    "- Simplifica enormemente los c√°lculos\n",
    "- A menudo da buenos resultados\n",
    "- Es especialmente bueno en an√°lisis de textos\n",
    "- Es **r√°pido** y necesita **pocos datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee2823",
   "metadata": {},
   "source": [
    "---\n",
    "## SECCI√ìN 3: Naive Bayes Gaussiano (Caracter√≠sticas Continuas)\n",
    "\n",
    "### 3.1 Distribuci√≥n Normal por clase\n",
    "\n",
    "Asumimos que cada caracter√≠stica sigue una distribuci√≥n normal dentro de cada clase:\n",
    "\n",
    "$$P(x_i|\\text{Clase}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c7829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset Iris\n",
    "iris = load_iris()\n",
    "X_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y_iris = pd.Series(iris.target, name=\"species\")\n",
    "\n",
    "# Visualizar distribuci√≥n de una caracter√≠stica por clase\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "feature_idx = 0\n",
    "feature_name = iris.feature_names[feature_idx]\n",
    "\n",
    "# Histograma para cada clase\n",
    "for class_idx in np.unique(y_iris):\n",
    "    data = X_iris[y_iris == class_idx][feature_name]\n",
    "    axes[0].hist(data, alpha=0.6, label=iris.target_names[class_idx], bins=15)\n",
    "\n",
    "axes[0].set_xlabel(feature_name)\n",
    "axes[0].set_ylabel('Frecuencia')\n",
    "axes[0].set_title('Distribuci√≥n de Sepal Length por Clase')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Curvas normales te√≥ricas\n",
    "x_range = np.linspace(X_iris[feature_name].min(), X_iris[feature_name].max(), 200)\n",
    "for class_idx in np.unique(y_iris):\n",
    "    data = X_iris[y_iris == class_idx][feature_name]\n",
    "    mu = data.mean()\n",
    "    sigma = data.std()\n",
    "    y_dist = norm.pdf(x_range, mu, sigma)\n",
    "    axes[1].plot(x_range, y_dist, lw=2, label=iris.target_names[class_idx])\n",
    "    axes[1].fill_between(x_range, y_dist, alpha=0.2)\n",
    "\n",
    "axes[1].set_xlabel(feature_name)\n",
    "axes[1].set_ylabel('Densidad de Probabilidad')\n",
    "axes[1].set_title('Distribuciones Normales (Asumidas por Naive Bayes)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Cada clase tiene su propia distribuci√≥n normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90caad5",
   "metadata": {},
   "source": [
    "### 3.2 Entrenamiento\n",
    "\n",
    "El entrenamiento solo calcula la media (Œº) y desviaci√≥n est√°ndar (œÉ) para cada caracter√≠stica en cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdeff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "# Entrenar modelo\n",
    "nb_gaussian = GaussianNB()\n",
    "nb_gaussian.fit(X_train, y_train)\n",
    "\n",
    "# Ver par√°metros aprendidos\n",
    "print(\"=\" * 60)\n",
    "print(\"PAR√ÅMETROS APRENDIDOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for class_idx, class_name in enumerate(iris.target_names):\n",
    "    print(f\"\\nüîπ Clase: {class_name}\")\n",
    "    print(f\"   Medias (Œº):\")\n",
    "    for feat_idx, feat_name in enumerate(iris.feature_names):\n",
    "        mu = nb_gaussian.theta_[class_idx, feat_idx]\n",
    "        print(f\"     {feat_name}: {mu:.3f}\")\n",
    "    \n",
    "    print(f\"   Desviaciones (œÉ):\")\n",
    "    for feat_idx, feat_name in enumerate(iris.feature_names):\n",
    "        sigma = np.sqrt(nb_gaussian.var_[class_idx, feat_idx])\n",
    "        print(f\"     {feat_name}: {sigma:.3f}\")\n",
    "    \n",
    "    prior = nb_gaussian.class_prior_[class_idx]\n",
    "    print(f\"   Prior P(Clase): {prior:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eef5b62",
   "metadata": {},
   "source": [
    "### 3.3 Predicci√≥n manual paso a paso\n",
    "\n",
    "Vamos a predecir la clase de un ejemplo calculando las probabilidades manualmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8733481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomar un ejemplo\n",
    "sample_idx = 5\n",
    "sample = X_test.iloc[sample_idx].values\n",
    "true_class = y_test.iloc[sample_idx]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREDICCI√ìN MANUAL CON TEOREMA DE BAYES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìä Flor a clasificar:\")\n",
    "for i, feat_name in enumerate(iris.feature_names):\n",
    "    print(f\"   {feat_name}: {sample[i]:.2f}\")\n",
    "print(f\"\\nClase real: {iris.target_names[true_class]}\")\n",
    "\n",
    "# Funci√≥n para calcular P(x | clase)\n",
    "def gaussian_prob(x, mu, sigma):\n",
    "    \"\"\"Calcula P(x|clase) usando distribuci√≥n normal\"\"\"\n",
    "    numerator = np.exp(-((x - mu)**2) / (2 * sigma**2))\n",
    "    denominator = np.sqrt(2 * np.pi * sigma**2)\n",
    "    return numerator / denominator\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"C√ÅLCULO DETALLADO POR CADA CLASE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "posteriors = []\n",
    "for class_idx in np.unique(y_train):\n",
    "    prior = np.log(nb_gaussian.class_prior_[class_idx])\n",
    "    likelihood = 0\n",
    "    \n",
    "    print(f\"\\nüî∏ Clase: {iris.target_names[class_idx]}\")\n",
    "    print(f\"   Prior: P(Clase) = {nb_gaussian.class_prior_[class_idx]:.4f}\")\n",
    "    print(f\"   Likelihoods (caracter√≠sticas):\")\n",
    "    \n",
    "    for feat_idx in range(len(sample)):\n",
    "        mu = nb_gaussian.theta_[class_idx, feat_idx]\n",
    "        sigma = np.sqrt(nb_gaussian.var_[class_idx, feat_idx])\n",
    "        x_val = sample[feat_idx]\n",
    "        \n",
    "        prob = gaussian_prob(x_val, mu, sigma)\n",
    "        likelihood += np.log(prob)\n",
    "        \n",
    "        print(f\"     {iris.feature_names[feat_idx]}: P({x_val:.2f}|clase) = {prob:.6f}\")\n",
    "    \n",
    "    posterior = prior + likelihood\n",
    "    posteriors.append(posterior)\n",
    "    print(f\"   Posterior total (log): {posterior:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTADO\")\n",
    "print(\"=\" * 70)\n",
    "best_class_idx = np.argmax(posteriors)\n",
    "print(f\"\\n‚úÖ PREDICCI√ìN: {iris.target_names[best_class_idx]}\")\n",
    "print(f\"   (Mayor log-posterior: {posteriors[best_class_idx]:.4f})\")\n",
    "print(f\"\\n‚úì Predicci√≥n correcta: {'S√ç ‚úÖ' if best_class_idx == true_class else 'NO ‚ùå'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0e837f",
   "metadata": {},
   "source": [
    "### 3.4 Probabilidades predichas\n",
    "\n",
    "El modelo tambi√©n devuelve probabilidades (no solo la clase predicha):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d3963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener probabilidades para nuevos ejemplos\n",
    "y_pred_proba = nb_gaussian.predict_proba(X_test)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROBABILIDADES PREDICHAS (PRIMEROS 5 EJEMPLOS)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\nüìå Ejemplo {i+1}:\")\n",
    "    print(f\"   Real: {iris.target_names[y_test.iloc[i]]}\")\n",
    "    for class_idx, class_name in enumerate(iris.target_names):\n",
    "        prob = y_pred_proba[i, class_idx]\n",
    "        bar = \"‚ñà\" * int(prob * 20)\n",
    "        print(f\"   {class_name:15s}: {prob:.4f} {bar}\")\n",
    "    pred = nb_gaussian.predict([X_test.iloc[i].values])[0]\n",
    "    print(f\"   ‚ûú Predicci√≥n: {iris.target_names[pred]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837689b3",
   "metadata": {},
   "source": [
    "### 3.5 Evaluaci√≥n del modelo Gaussiano\n",
    "\n",
    "M√©tricas est√°ndar de clasificaci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d704923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones\n",
    "y_pred = nb_gaussian.predict(X_test)\n",
    "\n",
    "# Evaluaci√≥n\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUACI√ìN EN TEST SET\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(\"\\nReporte de Clasificaci√≥n:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=iris.target_names,\n",
    "            yticklabels=iris.target_names)\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusi√≥n - Naive Bayes Gaussiano')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9790fd52",
   "metadata": {},
   "source": [
    "---\n",
    "## SECCI√ìN 4: Naive Bayes Multinomial (Para Textos)\n",
    "\n",
    "### 4.1 Problema: Clasificaci√≥n de rese√±as\n",
    "\n",
    "Queremos clasificar rese√±as de pel√≠culas como positivas o negativas bas√°ndonos en las palabras que contienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0dca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Rese√±as de pel√≠culas\n",
    "reviews = [\n",
    "    \"This movie is amazing and fantastic\",\n",
    "    \"I loved this film absolutely wonderful\",\n",
    "    \"Terrible movie waste of time\",\n",
    "    \"This is a bad and boring film\",\n",
    "    \"Great acting and excellent story\",\n",
    "    \"Horrible and disappointing\",\n",
    "    \"Best movie ever made\",\n",
    "    \"Worst film I have ever seen\"\n",
    "]\n",
    "\n",
    "labels = [1, 1, 0, 0, 1, 0, 1, 0]  # 1=Positivo, 0=Negativo\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATASET: RESE√ëAS DE PEL√çCULAS\")\n",
    "print(\"=\" * 70)\n",
    "for i, (review, label) in enumerate(zip(reviews, labels)):\n",
    "    sentiment = \"POSITIVO üü¢\" if label == 1 else \"NEGATIVO üî¥\"\n",
    "    print(f\"{i+1}. [{sentiment}] {review}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec11457",
   "metadata": {},
   "source": [
    "### 4.2 Vectorizaci√≥n: Bag of Words\n",
    "\n",
    "Convertimos cada texto en un vector de conteos de palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d24559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir textos a matriz de conteos\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "X_text = vectorizer.fit_transform(reviews)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"VOCABULARIO APRENDIDO\")\n",
    "print(\"=\" * 70)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f\"Palabras √∫nicas: {len(feature_names)}\")\n",
    "print(f\"Palabras: {', '.join(feature_names)}\")\n",
    "\n",
    "# Mostrar matriz de conteos\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MATRIZ DE CONTEOS (BAG OF WORDS)\")\n",
    "print(\"=\" * 70)\n",
    "df_counts = pd.DataFrame(\n",
    "    X_text.toarray(),\n",
    "    columns=feature_names\n",
    ")\n",
    "df_counts['Sentimiento'] = ['POSITIVO' if l == 1 else 'NEGATIVO' for l in labels]\n",
    "print(df_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ef05e",
   "metadata": {},
   "source": [
    "### 4.3 Entrenamiento Multinomial\n",
    "\n",
    "Naive Bayes Multinomial trabaja directamente con conteos de palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba4de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar\n",
    "nb_multi = MultinomialNB()\n",
    "nb_multi.fit(X_text, labels)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PAR√ÅMETROS APRENDIDOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Log-probabilidades por palabra y clase\n",
    "print(\"\\nüî¥ CLASE NEGATIVA (0):\")\n",
    "print(\"   Probabilidades de cada palabra:\")\n",
    "for word, prob in zip(feature_names, nb_multi.feature_log_prob_[0]):\n",
    "    print(f\"     {word:15s}: {np.exp(prob):.4f}\")\n",
    "\n",
    "print(\"\\nüü¢ CLASE POSITIVA (1):\")\n",
    "print(\"   Probabilidades de cada palabra:\")\n",
    "for word, prob in zip(feature_names, nb_multi.feature_log_prob_[1]):\n",
    "    print(f\"     {word:15s}: {np.exp(prob):.4f}\")\n",
    "\n",
    "# Priors\n",
    "print(f\"\\nüìä Probabilidades iniciales (priors):\")\n",
    "print(f\"   P(NEGATIVO) = {np.exp(nb_multi.class_log_prior_[0]):.4f}\")\n",
    "print(f\"   P(POSITIVO) = {np.exp(nb_multi.class_log_prior_[1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb14987",
   "metadata": {},
   "source": [
    "### 4.4 Predicci√≥n en textos nuevos\n",
    "\n",
    "Clasificamos textos que el modelo nunca ha visto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18125c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textos nuevos\n",
    "new_reviews = [\n",
    "    \"This movie is amazing\",\n",
    "    \"Terrible and boring film\"\n",
    "]\n",
    "\n",
    "# Vectorizar y predecir\n",
    "X_new = vectorizer.transform(new_reviews)\n",
    "predictions = nb_multi.predict(X_new)\n",
    "probabilities = nb_multi.predict_proba(X_new)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREDICCIONES EN TEXTOS NUEVOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for review, pred, probs in zip(new_reviews, predictions, probabilities):\n",
    "    sentiment = \"POSITIVO üü¢\" if pred == 1 else \"NEGATIVO üî¥\"\n",
    "    print(f\"\\nüìù Texto: \\\"{review}\\\"\")\n",
    "    print(f\"   Predicci√≥n: {sentiment}\")\n",
    "    print(f\"   P(NEGATIVO) = {probs[0]:.4f}\")\n",
    "    print(f\"   P(POSITIVO) = {probs[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1446b112",
   "metadata": {},
   "source": [
    "### 4.5 Palabras m√°s informativas\n",
    "\n",
    "¬øQu√© palabras son m√°s importantes para cada sentimiento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f99a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diferencia entre clases para cada palabra\n",
    "logprob_pos = nb_multi.feature_log_prob_[1]\n",
    "logprob_neg = nb_multi.feature_log_prob_[0]\n",
    "diff = logprob_pos - logprob_neg\n",
    "\n",
    "# Top 5 palabras positivas y negativas\n",
    "top_pos_idx = np.argsort(diff)[-5:]\n",
    "top_neg_idx = np.argsort(diff)[:5]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PALABRAS M√ÅS INFORMATIVAS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüü¢ PALABRAS M√ÅS ASOCIADAS A POSITIVO:\")\n",
    "for idx in reversed(top_pos_idx):\n",
    "    word = feature_names[idx]\n",
    "    print(f\"   {word}\")\n",
    "\n",
    "print(\"\\nüî¥ PALABRAS M√ÅS ASOCIADAS A NEGATIVO:\")\n",
    "for idx in top_neg_idx:\n",
    "    word = feature_names[idx]\n",
    "    print(f\"   {word}\")\n",
    "\n",
    "# Visualizar\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "words_pos = feature_names[top_pos_idx]\n",
    "diffs_pos = diff[top_pos_idx]\n",
    "ax1.barh(words_pos, diffs_pos, color='green', alpha=0.7)\n",
    "ax1.set_xlabel('Diferencia de Log-Probabilidad')\n",
    "ax1.set_title('Palabras Indicadoras de Rese√±a POSITIVA')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "words_neg = feature_names[top_neg_idx]\n",
    "diffs_neg = diff[top_neg_idx]\n",
    "ax2.barh(words_neg, diffs_neg, color='red', alpha=0.7)\n",
    "ax2.set_xlabel('Diferencia de Log-Probabilidad')\n",
    "ax2.set_title('Palabras Indicadoras de Rese√±a NEGATIVA')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6f39f6",
   "metadata": {},
   "source": [
    "---\n",
    "## SECCI√ìN 5: Caso Real - Detecci√≥n de Spam\n",
    "\n",
    "Clasificar emails como spam o leg√≠timos usando palabras clave:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bb208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la diferencia de log-probabilidades para cada palabra\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "logprob_pos = nb_multi.feature_log_prob_[1]\n",
    "logprob_neg = nb_multi.feature_log_prob_[0]\n",
    "\n",
    "# Palabras m√°s asociadas a positivo\n",
    "pos_diff = logprob_pos - logprob_neg\n",
    "top_pos_idx = np.argsort(pos_diff)[-5:]\n",
    "top_neg_idx = np.argsort(pos_diff)[:5]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PALABRAS M√ÅS INFORMATIVAS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüü¢ PALABRAS M√ÅS ASOCIADAS A POSITIVO:\")\n",
    "for idx in reversed(top_pos_idx):\n",
    "    word = feature_names[idx]\n",
    "    diff = pos_diff[idx]\n",
    "    print(f\"   {word}: diferencia = {diff:.4f}\")\n",
    "\n",
    "print(\"\\nüî¥ PALABRAS M√ÅS ASOCIADAS A NEGATIVO:\")\n",
    "for idx in top_neg_idx:\n",
    "    word = feature_names[idx]\n",
    "    diff = pos_diff[idx]\n",
    "    print(f\"   {word}: diferencia = {diff:.4f}\")\n",
    "\n",
    "# Visualizar\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Palabras positivas\n",
    "words_pos = feature_names[top_pos_idx]\n",
    "diffs_pos = pos_diff[top_pos_idx]\n",
    "ax1.barh(words_pos, diffs_pos, color='green', alpha=0.7)\n",
    "ax1.set_xlabel('Log-prob(Positivo) - Log-prob(Negativo)')\n",
    "ax1.set_title('Palabras Asociadas a Rese√±as Positivas')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Palabras negativas\n",
    "words_neg = feature_names[top_neg_idx]\n",
    "diffs_neg = pos_diff[top_neg_idx]\n",
    "ax2.barh(words_neg, diffs_neg, color='red', alpha=0.7)\n",
    "ax2.set_xlabel('Log-prob(Positivo) - Log-prob(Negativo)')\n",
    "ax2.set_title('Palabras Asociadas a Rese√±as Negativas')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45080719",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. PARTE III: Caso Real - An√°lisis de Spam\n",
    "\n",
    "Usaremos un dataset real de emails spam vs leg√≠timos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones en test set\n",
    "y_pred = nb_gaussian.predict(X_test)\n",
    "\n",
    "# Evaluaci√≥n\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUACI√ìN EN TEST SET\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(\"\\nReporte de Clasificaci√≥n:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=iris.target_names,\n",
    "            yticklabels=iris.target_names)\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusi√≥n - Naive Bayes Gaussiano (Iris)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aad834",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. PARTE II: Naive Bayes Multinomial (Para Textos y Conteos)\n",
    "\n",
    "### 4.1 Problema: Clasificaci√≥n de Textos\n",
    "\n",
    "Queremos clasificar textos seg√∫n su sentimiento (positivo/negativo) o detectar spam.\n",
    "\n",
    "**Idea:** Contar la frecuencia de palabras en cada texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849775e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de ejemplo: Rese√±as de pel√≠culas\n",
    "reviews = [\n",
    "    \"This movie is amazing and fantastic\",\n",
    "    \"I loved this film, absolutely wonderful\",\n",
    "    \"Terrible movie, waste of time\",\n",
    "    \"This is a bad and boring film\",\n",
    "    \"Great acting and excellent story\",\n",
    "    \"Horrible and disappointing\",\n",
    "    \"Best movie ever made\",\n",
    "    \"Worst film I have ever seen\"\n",
    "]\n",
    "\n",
    "# Labels: 1 = Positivo, 0 = Negativo\n",
    "labels = [1, 1, 0, 0, 1, 0, 1, 0]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATASET: RESE√ëAS DE PEL√çCULAS\")\n",
    "print(\"=\" * 70)\n",
    "for i, (review, label) in enumerate(zip(reviews, labels)):\n",
    "    sentiment = \"POSITIVO\" if label == 1 else \"NEGATIVO\"\n",
    "    print(f\"{i+1}. [{sentiment}] {review}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c473444",
   "metadata": {},
   "source": [
    "### 4.2 Vectorizar el texto: Bag of Words\n",
    "\n",
    "Convertimos cada texto en un vector de conteos de palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e78750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir textos a matriz de conteos\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "X_text = vectorizer.fit_transform(reviews)\n",
    "\n",
    "# Ver vocabulario\n",
    "print(\"=\" * 70)\n",
    "print(\"VOCABULARIO APRENDIDO\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Palabras √∫nicas: {len(vectorizer.get_feature_names_out())}\")\n",
    "print(f\"Palabras: {', '.join(vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "# Mostrar matriz de conteos\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MATRIZ DE CONTEOS (BAG OF WORDS)\")\n",
    "print(\"=\" * 70)\n",
    "df_counts = pd.DataFrame(\n",
    "    X_text.toarray(),\n",
    "    columns=vectorizer.get_feature_names_out()\n",
    ")\n",
    "df_counts['Label'] = ['POS' if l == 1 else 'NEG' for l in labels]\n",
    "print(df_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f87e20",
   "metadata": {},
   "source": [
    "### 4.3 Entrenar Naive Bayes Multinomial\n",
    "\n",
    "Para textos, usamos Naive Bayes Multinomial que trabaja directamente con conteos de palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar Naive Bayes Multinomial\n",
    "nb_multi = MultinomialNB()\n",
    "nb_multi.fit(X_text, labels)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PAR√ÅMETROS APRENDIDOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Log-probabilidades aprendidas\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"\\nLog-probabilidades de cada palabra por clase:\")\n",
    "print(f\"\\nüî∏ Clase NEGATIVA (0):\")\n",
    "for word, prob in zip(feature_names, nb_multi.feature_log_prob_[0]):\n",
    "    print(f\"   {word}: {prob:.4f} (prob = {np.exp(prob):.4f})\")\n",
    "\n",
    "print(f\"\\nüî∏ Clase POSITIVA (1):\")\n",
    "for word, prob in zip(feature_names, nb_multi.feature_log_prob_[1]):\n",
    "    print(f\"   {word}: {prob:.4f} (prob = {np.exp(prob):.4f})\")\n",
    "\n",
    "# Prior de cada clase\n",
    "print(f\"\\nPrior (ocurrencia de cada clase):\")\n",
    "print(f\"   P(NEGATIVO) = {nb_multi.class_log_prior_[0]:.4f} (prob = {np.exp(nb_multi.class_log_prior_[0]):.4f})\")\n",
    "print(f\"   P(POSITIVO) = {nb_multi.class_log_prior_[1]:.4f} (prob = {np.exp(nb_multi.class_log_prior_[1]):.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Predicci√≥n en textos nuevos\n",
    "\n",
    "Veamos c√≥mo el modelo clasifica textos que no ha visto:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cf42c6",
   "metadata": {},
   "source": [
    "### 1.2 Probabilidad Condicional\n",
    "\n",
    "**Probabilidad condicional**: Probabilidad de A sabiendo que B ya ocurri√≥.\n",
    "\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
    "\n",
    "**Ejemplo en medicina:**\n",
    "- P(Enfermo | Test positivo) = ?\n",
    "- Necesitamos saber:\n",
    "  - Qu√© probabilidad tiene un enfermo de dar test positivo\n",
    "  - Qu√© probabilidad tiene un sano de dar test positivo (falso positivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6775e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Test de enfermedad\n",
    "# P(Enfermo) = 0.01 (1% de la poblaci√≥n est√° enferma)\n",
    "# P(Test+ | Enfermo) = 0.99 (99% de sensibilidad)\n",
    "# P(Test+ | Sano) = 0.05 (5% de falsos positivos)\n",
    "\n",
    "P_enfermo = 0.01\n",
    "P_sano = 0.99\n",
    "P_test_pos_dado_enfermo = 0.99\n",
    "P_test_pos_dado_sano = 0.05\n",
    "\n",
    "# Probabilidad de dar test positivo\n",
    "P_test_pos = (P_test_pos_dado_enfermo * P_enfermo) + (P_test_pos_dado_sano * P_sano)\n",
    "\n",
    "# TEOREMA DE BAYES: P(Enfermo | Test+)\n",
    "P_enfermo_dado_test_pos = (P_test_pos_dado_enfermo * P_enfermo) / P_test_pos\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EJEMPLO: TEST DE ENFERMEDAD\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDatos iniciales:\")\n",
    "print(f\"  P(Enfermo) = {P_enfermo:.1%}\")\n",
    "print(f\"  P(Test+ | Enfermo) = {P_test_pos_dado_enfermo:.1%}  (sensibilidad)\")\n",
    "print(f\"  P(Test+ | Sano) = {P_test_pos_dado_sano:.1%}       (falso positivo)\")\n",
    "\n",
    "print(f\"\\nC√°lculos:\")\n",
    "print(f\"  P(Test+) = {P_test_pos:.4f}\")\n",
    "print(f\"  P(Enfermo | Test+) = {P_enfermo_dado_test_pos:.4f} = {P_enfermo_dado_test_pos:.1%}\")\n",
    "\n",
    "print(f\"\\nüí° Interpretaci√≥n:\")\n",
    "print(f\"  Incluso si das positivo, solo hay {P_enfermo_dado_test_pos:.1%} de probabilidad de estar enfermo\")\n",
    "print(f\"  ¬øPor qu√©? Porque la enfermedad es rara (1% en poblaci√≥n)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81ab8a8",
   "metadata": {},
   "source": [
    "### 1.3 El Teorema de Bayes (La Joya de la Corona)\n",
    "\n",
    "El Teorema de Bayes nos permite calcular probabilidades \"hacia atr√°s\":\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\n",
    "\n",
    "**En contexto de clasificaci√≥n:**\n",
    "$$P(\\text{Clase}|\\text{Datos}) = \\frac{P(\\text{Datos}|\\text{Clase}) \\cdot P(\\text{Clase})}{P(\\text{Datos})}$$\n",
    "\n",
    "**T√©rmino a t√©rmino:**\n",
    "- **P(Clase | Datos)**: Lo que queremos saber (probabilidad posterior)\n",
    "- **P(Datos | Clase)**: Verosimilitud (likelihood)\n",
    "- **P(Clase)**: Probabilidad previa (prior)\n",
    "- **P(Datos)**: Evidencia (normalizador)\n",
    "\n",
    "---\n",
    "## 2. Naive Bayes: La Suposici√≥n Ingenua\n",
    "\n",
    "### 2.1 ¬øPor qu√© \"Naive\"?\n",
    "\n",
    "Naive Bayes **asume que todas las caracter√≠sticas son independientes** dado la clase.\n",
    "\n",
    "$$P(\\text{Datos}|\\text{Clase}) = P(x_1|\\text{Clase}) \\cdot P(x_2|\\text{Clase}) \\cdot ... \\cdot P(x_n|\\text{Clase})$$\n",
    "\n",
    "**¬øEs realista esta suposici√≥n?** ‚ùå NO, muchas caracter√≠sticas est√°n correlacionadas.\n",
    "\n",
    "**¬øEntonces por qu√© funciona?** ‚úÖ Porque:\n",
    "1. Simplifica enormemente los c√°lculos\n",
    "2. A menudo da buenos resultados en la pr√°ctica\n",
    "3. Es especialmente bueno en textos (palabras casi independientes)\n",
    "4. Es **r√°pido** y necesita **poco datos**\n",
    "\n",
    "---\n",
    "## 3. PARTE I: Naive Bayes Gaussiano (Caracter√≠sticas Continuas)\n",
    "\n",
    "### 3.1 Idea: Distribuci√≥n Normal\n",
    "\n",
    "Asumimos que cada caracter√≠stica sigue una **distribuci√≥n normal** dentro de cada clase:\n",
    "\n",
    "$$P(x_i|\\text{Clase}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "Donde:\n",
    "- $\\mu$ = media de la caracter√≠stica en esa clase\n",
    "- $\\sigma$ = desviaci√≥n est√°ndar en esa clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea00a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset Iris\n",
    "iris = load_iris()\n",
    "X_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y_iris = pd.Series(iris.target, name=\"species\")\n",
    "\n",
    "# Visualizar distribuci√≥n de una caracter√≠stica por clase\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "feature_idx = 0  # Sepal length\n",
    "feature_name = iris.feature_names[feature_idx]\n",
    "\n",
    "# Graficar histograma para cada clase\n",
    "for class_idx in np.unique(y_iris):\n",
    "    data = X_iris[y_iris == class_idx][feature_name]\n",
    "    axes[0].hist(data, alpha=0.6, label=iris.target_names[class_idx], bins=15)\n",
    "\n",
    "axes[0].set_xlabel(feature_name)\n",
    "axes[0].set_ylabel('Frecuencia')\n",
    "axes[0].set_title('Distribuci√≥n de Sepal Length por Clase')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Graficar curvas normales\n",
    "x_range = np.linspace(X_iris[feature_name].min(), X_iris[feature_name].max(), 200)\n",
    "for class_idx in np.unique(y_iris):\n",
    "    data = X_iris[y_iris == class_idx][feature_name]\n",
    "    mu = data.mean()\n",
    "    sigma = data.std()\n",
    "    \n",
    "    # Funci√≥n de densidad normal\n",
    "    y_dist = norm.pdf(x_range, mu, sigma)\n",
    "    axes[1].plot(x_range, y_dist, lw=2, label=iris.target_names[class_idx])\n",
    "    axes[1].fill_between(x_range, y_dist, alpha=0.2)\n",
    "\n",
    "axes[1].set_xlabel(feature_name)\n",
    "axes[1].set_ylabel('Densidad de Probabilidad')\n",
    "axes[1].set_title('Distribuci√≥n Normal Asumida por Naive Bayes')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Observa c√≥mo cada clase tiene su propia distribuci√≥n normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c201934",
   "metadata": {},
   "source": [
    "### 3.2 Entrenar Naive Bayes Gaussiano\n",
    "\n",
    "El entrenamiento es muy simple: solo calcula $\\mu$ y $\\sigma$ para cada caracter√≠stica en cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66c49fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "# Entrenar Naive Bayes Gaussiano\n",
    "nb_gaussian = GaussianNB()\n",
    "nb_gaussian.fit(X_train, y_train)\n",
    "\n",
    "# El modelo aprendi√≥ las medias y desviaciones\n",
    "print(\"=\" * 60)\n",
    "print(\"PAR√ÅMETROS APRENDIDOS POR NAIVE BAYES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for class_idx, class_name in enumerate(iris.target_names):\n",
    "    print(f\"\\nüîπ Clase: {class_name}\")\n",
    "    print(f\"   Medias (Œº):\")\n",
    "    for feat_idx, feat_name in enumerate(iris.feature_names):\n",
    "        print(f\"     {feat_name}: {nb_gaussian.theta_[class_idx, feat_idx]:.3f}\")\n",
    "    \n",
    "    print(f\"   Desviaciones est√°ndar (œÉ):\")\n",
    "    for feat_idx, feat_name in enumerate(iris.feature_names):\n",
    "        sigma = np.sqrt(nb_gaussian.var_[class_idx, feat_idx])\n",
    "        print(f\"     {feat_name}: {sigma:.3f}\")\n",
    "    \n",
    "    print(f\"   Prior P(Clase): {nb_gaussian.class_prior_[class_idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Predicci√≥n: Calculando probabilidades manualmente\n",
    "\n",
    "Vamos a predecir la clase de un ejemplo step-by-step usando Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomar un ejemplo del test set\n",
    "sample_idx = 0\n",
    "sample = X_test.iloc[sample_idx].values\n",
    "true_class = y_test.iloc[sample_idx]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREDICCI√ìN MANUAL CON TEOREMA DE BAYES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìä Ejemplo a clasificar:\")\n",
    "for i, feat_name in enumerate(iris.feature_names):\n",
    "    print(f\"   {feat_name}: {sample[i]:.3f}\")\n",
    "\n",
    "print(f\"\\nClase real: {iris.target_names[true_class]}\")\n",
    "\n",
    "# Calcular P(caracter√≠sticas | clase) para cada clase\n",
    "def gaussian_prob(x, mu, sigma):\n",
    "    \"\"\"Calcula P(x|clase) usando distribuci√≥n normal\"\"\"\n",
    "    numerator = np.exp(-((x - mu)**2) / (2 * sigma**2))\n",
    "    denominator = np.sqrt(2 * np.pi * sigma**2)\n",
    "    return numerator / denominator\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"C√ÅLCULO DETALLADO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "posteriors = []\n",
    "for class_idx in np.unique(y_train):\n",
    "    prior = np.log(nb_gaussian.class_prior_[class_idx])\n",
    "    likelihood = 0\n",
    "    \n",
    "    print(f\"\\nüî∏ Clase: {iris.target_names[class_idx]}\")\n",
    "    print(f\"   Prior P(Clase) = {nb_gaussian.class_prior_[class_idx]:.4f}\")\n",
    "    print(f\"   log(Prior) = {prior:.4f}\")\n",
    "    \n",
    "    print(f\"   Likelihoods P(caracter√≠sticas|Clase):\")\n",
    "    \n",
    "    for feat_idx in range(len(sample)):\n",
    "        mu = nb_gaussian.theta_[class_idx, feat_idx]\n",
    "        sigma = np.sqrt(nb_gaussian.var_[class_idx, feat_idx])\n",
    "        x_val = sample[feat_idx]\n",
    "        \n",
    "        prob = gaussian_prob(x_val, mu, sigma)\n",
    "        likelihood += np.log(prob)\n",
    "        \n",
    "        print(f\"      {iris.feature_names[feat_idx]}: P({x_val:.3f}|clase) = {prob:.6f}, log = {np.log(prob):.4f}\")\n",
    "    \n",
    "    # Posterior (sin normalizar, pero suficiente para comparar)\n",
    "    posterior = prior + likelihood\n",
    "    posteriors.append(posterior)\n",
    "    \n",
    "    print(f\"   log(Posterior) = {posterior:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTADO\")\n",
    "print(\"=\" * 70)\n",
    "best_class_idx = np.argmax(posteriors)\n",
    "print(f\"\\n‚úÖ Predicci√≥n: {iris.target_names[best_class_idx]}\")\n",
    "print(f\"   (Mayor log-posterior: {posteriors[best_class_idx]:.4f})\")\n",
    "\n",
    "# Comparar con predicci√≥n del modelo\n",
    "pred_model = nb_gaussian.predict([sample])[0]\n",
    "print(f\"\\nü§ñ Predicci√≥n del modelo: {iris.target_names[pred_model]}\")\n",
    "print(f\"   Coincide: {'‚úÖ S√ç' if pred_model == best_class_idx else '‚ùå NO'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Probabilidades predichas\n",
    "\n",
    "Naive Bayes tambi√©n puede darnos las probabilidades de cada clase (no solo la clase):"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
