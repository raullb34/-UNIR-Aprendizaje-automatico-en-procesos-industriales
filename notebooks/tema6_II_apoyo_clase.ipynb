{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69b5667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.datasets import make_classification, make_circles, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417eaa1e",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. PARTE I: SVM Lineal (Datos Linealmente Separables)\n",
    "\n",
    "### 3.1 Generar datos sint√©ticos linealmente separables\n",
    "\n",
    "Crearemos un dataset simple con 2 clases que se pueden separar con una l√≠nea recta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac44f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos linealmente separables\n",
    "np.random.seed(42)\n",
    "X_linear, y_linear = make_classification(\n",
    "    n_samples=100,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=2.0,  # Separaci√≥n clara entre clases\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Visualizar los datos\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_linear[y_linear == 0][:, 0], X_linear[y_linear == 0][:, 1], \n",
    "            c='blue', label='Clase 0', alpha=0.7, edgecolors='k')\n",
    "plt.scatter(X_linear[y_linear == 1][:, 0], X_linear[y_linear == 1][:, 1], \n",
    "            c='red', label='Clase 1', alpha=0.7, edgecolors='k')\n",
    "plt.xlabel('Caracter√≠stica 1')\n",
    "plt.ylabel('Caracter√≠stica 2')\n",
    "plt.title('Dataset Linealmente Separable')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d29c5e7",
   "metadata": {},
   "source": [
    "### 3.2 Entrenar SVM Lineal\n",
    "\n",
    "Usamos un **kernel lineal** (`kernel='linear'`) porque los datos son linealmente separables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9fdf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en train/test\n",
    "X_train_lin, X_test_lin, y_train_lin, y_test_lin = train_test_split(\n",
    "    X_linear, y_linear, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Escalar los datos (importante para SVM)\n",
    "scaler_lin = StandardScaler()\n",
    "X_train_lin_scaled = scaler_lin.fit_transform(X_train_lin)\n",
    "X_test_lin_scaled = scaler_lin.transform(X_test_lin)\n",
    "\n",
    "# Crear y entrenar SVM lineal\n",
    "svm_linear = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm_linear.fit(X_train_lin_scaled, y_train_lin)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_lin = svm_linear.predict(X_test_lin_scaled)\n",
    "\n",
    "# Evaluaci√≥n\n",
    "print(\"=\" * 50)\n",
    "print(\"RESULTADOS SVM LINEAL\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy: {accuracy_score(y_test_lin, y_pred_lin):.3f}\")\n",
    "print(\"\\nReporte de Clasificaci√≥n:\")\n",
    "print(classification_report(y_test_lin, y_pred_lin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2bac61",
   "metadata": {},
   "source": [
    "### 3.3 Visualizar el hiperplano y los vectores de soporte\n",
    "\n",
    "Esta funci√≥n dibuja:\n",
    "- El hiperplano de separaci√≥n (l√≠nea negra)\n",
    "- Los m√°rgenes (l√≠neas punteadas)\n",
    "- Los vectores de soporte (puntos rodeados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5c7dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm_decision_boundary(X, y, model, title=\"SVM Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    Visualiza la frontera de decisi√≥n de un modelo SVM.\n",
    "    Dibuja el hiperplano, los m√°rgenes y resalta los vectores de soporte.\n",
    "    \"\"\"\n",
    "    # Crear una malla de puntos para graficar la frontera\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Predecir en cada punto de la malla\n",
    "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Graficar\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    # Frontera de decisi√≥n y m√°rgenes\n",
    "    plt.contourf(xx, yy, Z, levels=[-1, 0, 1], alpha=0.2, \n",
    "                 colors=['blue', 'white', 'red'])\n",
    "    plt.contour(xx, yy, Z, levels=[-1, 0, 1], linewidths=[1, 2, 1],\n",
    "                colors=['blue', 'black', 'red'], linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # Puntos de datos\n",
    "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], \n",
    "                c='blue', label='Clase 0', alpha=0.6, edgecolors='k', s=80)\n",
    "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], \n",
    "                c='red', label='Clase 1', alpha=0.6, edgecolors='k', s=80)\n",
    "    \n",
    "    # Vectores de soporte (resaltados)\n",
    "    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "                s=200, linewidth=2, facecolors='none', edgecolors='green',\n",
    "                label=f'Vectores de Soporte ({len(model.support_vectors_)})')\n",
    "    \n",
    "    plt.xlabel('Caracter√≠stica 1')\n",
    "    plt.ylabel('Caracter√≠stica 2')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar el modelo lineal\n",
    "plot_svm_decision_boundary(\n",
    "    X_train_lin_scaled, \n",
    "    y_train_lin, \n",
    "    svm_linear,\n",
    "    title=\"SVM Lineal - Hiperplano y Vectores de Soporte\"\n",
    ")\n",
    "\n",
    "print(f\"\\nN√∫mero de vectores de soporte: {len(svm_linear.support_vectors_)}\")\n",
    "print(f\"De {len(X_train_lin_scaled)} puntos de entrenamiento, solo {len(svm_linear.support_vectors_)} determinan el hiperplano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91116b28",
   "metadata": {},
   "source": [
    "### 3.4 Efecto del par√°metro C (regularizaci√≥n)\n",
    "\n",
    "El par√°metro **C** controla el trade-off entre:\n",
    "- **C grande** ‚Üí Margen estrecho, menos errores en entrenamiento (riesgo de overfitting)\n",
    "- **C peque√±o** ‚Üí Margen amplio, permite algunos errores (m√°s generalizaci√≥n)\n",
    "\n",
    "Vamos a comparar diferentes valores de C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6454c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar con diferentes valores de C\n",
    "C_values = [0.01, 1.0, 100.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, C in enumerate(C_values):\n",
    "    # Entrenar modelo\n",
    "    svm_c = SVC(kernel='linear', C=C, random_state=42)\n",
    "    svm_c.fit(X_train_lin_scaled, y_train_lin)\n",
    "    \n",
    "    # Crear malla\n",
    "    x_min, x_max = X_train_lin_scaled[:, 0].min() - 1, X_train_lin_scaled[:, 0].max() + 1\n",
    "    y_min, y_max = X_train_lin_scaled[:, 1].min() - 1, X_train_lin_scaled[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    Z = svm_c.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Graficar\n",
    "    ax = axes[idx]\n",
    "    ax.contourf(xx, yy, Z, levels=[-1, 0, 1], alpha=0.2, colors=['blue', 'white', 'red'])\n",
    "    ax.contour(xx, yy, Z, levels=[-1, 0, 1], linewidths=[1, 2, 1],\n",
    "               colors=['blue', 'black', 'red'], linestyles=['--', '-', '--'])\n",
    "    \n",
    "    ax.scatter(X_train_lin_scaled[y_train_lin == 0][:, 0], \n",
    "               X_train_lin_scaled[y_train_lin == 0][:, 1],\n",
    "               c='blue', alpha=0.6, edgecolors='k', s=60)\n",
    "    ax.scatter(X_train_lin_scaled[y_train_lin == 1][:, 0], \n",
    "               X_train_lin_scaled[y_train_lin == 1][:, 1],\n",
    "               c='red', alpha=0.6, edgecolors='k', s=60)\n",
    "    \n",
    "    ax.scatter(svm_c.support_vectors_[:, 0], svm_c.support_vectors_[:, 1],\n",
    "               s=150, linewidth=2, facecolors='none', edgecolors='green')\n",
    "    \n",
    "    ax.set_title(f'C = {C}\\n{len(svm_c.support_vectors_)} vectores de soporte')\n",
    "    ax.set_xlabel('Caracter√≠stica 1')\n",
    "    ax.set_ylabel('Caracter√≠stica 2')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observa c√≥mo cambia el n√∫mero de vectores de soporte y el margen con diferentes valores de C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377e80dd",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. PARTE II: SVM con Kernel (Datos NO Linealmente Separables)\n",
    "\n",
    "### 4.1 El problema: datos con patr√≥n circular\n",
    "\n",
    "Muchos problemas del mundo real no son linealmente separables. Por ejemplo, datos con patr√≥n circular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb84ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos con patr√≥n circular (NO linealmente separables)\n",
    "np.random.seed(42)\n",
    "X_circles, y_circles = make_circles(\n",
    "    n_samples=300,\n",
    "    noise=0.1,\n",
    "    factor=0.5,  # Radio relativo entre c√≠rculos\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_circles[y_circles == 0][:, 0], X_circles[y_circles == 0][:, 1],\n",
    "            c='blue', label='Clase 0 (c√≠rculo exterior)', alpha=0.7, edgecolors='k')\n",
    "plt.scatter(X_circles[y_circles == 1][:, 0], X_circles[y_circles == 1][:, 1],\n",
    "            c='red', label='Clase 1 (c√≠rculo interior)', alpha=0.7, edgecolors='k')\n",
    "plt.xlabel('Caracter√≠stica 1')\n",
    "plt.ylabel('Caracter√≠stica 2')\n",
    "plt.title('Dataset con Patr√≥n Circular (NO linealmente separable)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(\"¬øSe puede separar con una l√≠nea recta? ‚ùå NO\")\n",
    "print(\"Necesitamos el Kernel Trick üé©‚ú®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c3ec7a",
   "metadata": {},
   "source": [
    "### 4.2 SVM Lineal vs SVM con Kernel RBF\n",
    "\n",
    "Comparemos qu√© pasa si intentamos usar un kernel lineal vs un kernel RBF (Radial Basis Function).\n",
    "\n",
    "**Kernel RBF**: Transforma los datos a un espacio de mayor dimensi√≥n donde S√ç son linealmente separables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e3cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir datos\n",
    "X_train_circ, X_test_circ, y_train_circ, y_test_circ = train_test_split(\n",
    "    X_circles, y_circles, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Escalar datos\n",
    "scaler_circ = StandardScaler()\n",
    "X_train_circ_scaled = scaler_circ.fit_transform(X_train_circ)\n",
    "X_test_circ_scaled = scaler_circ.transform(X_test_circ)\n",
    "\n",
    "# Entrenar con kernel LINEAL (va a fallar)\n",
    "svm_linear_fail = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm_linear_fail.fit(X_train_circ_scaled, y_train_circ)\n",
    "y_pred_lin_fail = svm_linear_fail.predict(X_test_circ_scaled)\n",
    "\n",
    "# Entrenar con kernel RBF (va a funcionar bien)\n",
    "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_rbf.fit(X_train_circ_scaled, y_train_circ)\n",
    "y_pred_rbf = svm_rbf.predict(X_test_circ_scaled)\n",
    "\n",
    "# Comparar resultados\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARACI√ìN: SVM LINEAL vs SVM RBF (datos circulares)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüìâ SVM LINEAL (no apto para este problema):\")\n",
    "print(f\"   Accuracy: {accuracy_score(y_test_circ, y_pred_lin_fail):.3f}\")\n",
    "\n",
    "print(\"\\nüìà SVM con Kernel RBF (dise√±ado para este problema):\")\n",
    "print(f\"   Accuracy: {accuracy_score(y_test_circ, y_pred_rbf):.3f}\")\n",
    "print(\"\\nReporte de Clasificaci√≥n (RBF):\")\n",
    "print(classification_report(y_test_circ, y_pred_rbf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a0256",
   "metadata": {},
   "source": [
    "### 4.3 Visualizar las fronteras de decisi√≥n\n",
    "\n",
    "Veamos gr√°ficamente por qu√© el kernel RBF funciona mucho mejor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Funci√≥n para graficar frontera de decisi√≥n\n",
    "def plot_decision_boundary_circles(ax, X, y, model, title):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "    ax.scatter(X[y == 0][:, 0], X[y == 0][:, 1], c='blue', alpha=0.6, edgecolors='k', s=60)\n",
    "    ax.scatter(X[y == 1][:, 0], X[y == 1][:, 1], c='red', alpha=0.6, edgecolors='k', s=60)\n",
    "    ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "               s=150, linewidth=2, facecolors='none', edgecolors='green',\n",
    "               label=f'Vectores de soporte ({len(model.support_vectors_)})')\n",
    "    ax.set_xlabel('Caracter√≠stica 1')\n",
    "    ax.set_ylabel('Caracter√≠stica 2')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axis('equal')\n",
    "\n",
    "# Graficar ambos modelos\n",
    "plot_decision_boundary_circles(\n",
    "    axes[0], X_train_circ_scaled, y_train_circ, svm_linear_fail,\n",
    "    f\"SVM Lineal\\nAccuracy: {accuracy_score(y_test_circ, y_pred_lin_fail):.3f}\"\n",
    ")\n",
    "\n",
    "plot_decision_boundary_circles(\n",
    "    axes[1], X_train_circ_scaled, y_train_circ, svm_rbf,\n",
    "    f\"SVM con Kernel RBF\\nAccuracy: {accuracy_score(y_test_circ, y_pred_rbf):.3f}\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observa c√≥mo el kernel RBF puede crear una frontera circular\")\n",
    "print(\"   mientras que el lineal solo puede trazar una l√≠nea recta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c3a97c",
   "metadata": {},
   "source": [
    "### 4.4 El par√°metro gamma en Kernel RBF\n",
    "\n",
    "**Gamma** controla cu√°nta influencia tiene cada punto de entrenamiento:\n",
    "- **Gamma alto** ‚Üí Influencia local (riesgo de overfitting)\n",
    "- **Gamma bajo** ‚Üí Influencia amplia (frontera m√°s suave)\n",
    "\n",
    "Vamos a comparar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d92a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_values = [0.1, 1.0, 10.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, gamma in enumerate(gamma_values):\n",
    "    # Entrenar modelo\n",
    "    svm_g = SVC(kernel='rbf', C=1.0, gamma=gamma, random_state=42)\n",
    "    svm_g.fit(X_train_circ_scaled, y_train_circ)\n",
    "    y_pred_g = svm_g.predict(X_test_circ_scaled)\n",
    "    \n",
    "    # Graficar\n",
    "    plot_decision_boundary_circles(\n",
    "        axes[idx], X_train_circ_scaled, y_train_circ, svm_g,\n",
    "        f\"Gamma = {gamma}\\nAccuracy Test: {accuracy_score(y_test_circ, y_pred_g):.3f}\"\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Conclusi√≥n sobre Gamma:\")\n",
    "print(\"   - Gamma muy bajo (0.1): Frontera demasiado suave, underfitting\")\n",
    "print(\"   - Gamma medio (1.0): Balance √≥ptimo\")\n",
    "print(\"   - Gamma muy alto (10.0): Frontera muy irregular, overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc88f16",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. PARTE III: Caso Real - Detecci√≥n de C√°ncer de Mama\n",
    "\n",
    "Ahora aplicaremos SVM a un dataset real: **Breast Cancer Wisconsin**.\n",
    "\n",
    "**Objetivo**: Clasificar tumores como benignos (0) o malignos (1) bas√°ndose en caracter√≠sticas de las c√©lulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412c0cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset real\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "y_cancer = pd.Series(cancer.target, name=\"diagnosis\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET: BREAST CANCER WISCONSIN\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"N√∫mero de muestras: {len(X_cancer)}\")\n",
    "print(f\"N√∫mero de caracter√≠sticas: {X_cancer.shape[1]}\")\n",
    "print(f\"\\nDistribuci√≥n de clases:\")\n",
    "print(f\"  Clase 0 (Maligno): {(y_cancer == 0).sum()} ({(y_cancer == 0).sum()/len(y_cancer)*100:.1f}%)\")\n",
    "print(f\"  Clase 1 (Benigno): {(y_cancer == 1).sum()} ({(y_cancer == 1).sum()/len(y_cancer)*100:.1f}%)\")\n",
    "print(f\"\\nPrimeras caracter√≠sticas:\")\n",
    "print(X_cancer.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8390d1bf",
   "metadata": {},
   "source": [
    "### 5.1 Preprocesamiento y divisi√≥n de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ed4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en train/test\n",
    "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
    ")\n",
    "\n",
    "# Escalar caracter√≠sticas (CR√çTICO para SVM)\n",
    "scaler_cancer = StandardScaler()\n",
    "X_train_cancer_scaled = scaler_cancer.fit_transform(X_train_cancer)\n",
    "X_test_cancer_scaled = scaler_cancer.transform(X_test_cancer)\n",
    "\n",
    "print(\"Datos preparados:\")\n",
    "print(f\"  Train: {X_train_cancer_scaled.shape}\")\n",
    "print(f\"  Test: {X_test_cancer_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c0b28",
   "metadata": {},
   "source": [
    "### 5.2 B√∫squeda de hiperpar√°metros √≥ptimos con GridSearchCV\n",
    "\n",
    "Usaremos validaci√≥n cruzada para encontrar los mejores valores de C y gamma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d807acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir grid de hiperpar√°metros\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# GridSearchCV con validaci√≥n cruzada\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Buscando mejores hiperpar√°metros...\")\n",
    "grid_search.fit(X_train_cancer_scaled, y_train_cancer)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTADOS DE GRID SEARCH\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mejores par√°metros: {grid_search.best_params_}\")\n",
    "print(f\"Mejor score CV: {grid_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb94078",
   "metadata": {},
   "source": [
    "### 5.3 Evaluar el modelo optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa72b1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejor modelo\n",
    "best_svm = grid_search.best_estimator_\n",
    "\n",
    "# Predicciones\n",
    "y_pred_cancer = best_svm.predict(X_test_cancer_scaled)\n",
    "y_pred_proba_cancer = best_svm.decision_function(X_test_cancer_scaled)\n",
    "\n",
    "# M√©tricas\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUACI√ìN EN TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {accuracy_score(y_test_cancer, y_pred_cancer):.3f}\")\n",
    "print(\"\\nReporte de Clasificaci√≥n:\")\n",
    "print(classification_report(y_test_cancer, y_pred_cancer, \n",
    "                          target_names=['Maligno', 'Benigno']))\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "cm_cancer = confusion_matrix(y_test_cancer, y_pred_cancer)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_cancer, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Maligno', 'Benigno'],\n",
    "            yticklabels=['Maligno', 'Benigno'])\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusi√≥n - SVM en Breast Cancer')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Interpretaci√≥n:\")\n",
    "print(f\"   - Verdaderos Negativos (TN): {cm_cancer[0,0]} (malignos correctamente identificados)\")\n",
    "print(f\"   - Falsos Positivos (FP): {cm_cancer[0,1]} (benignos clasificados como malignos)\")\n",
    "print(f\"   - Falsos Negativos (FN): {cm_cancer[1,0]} (malignos clasificados como benignos ‚ö†Ô∏è CR√çTICO)\")\n",
    "print(f\"   - Verdaderos Positivos (TP): {cm_cancer[1,1]} (benignos correctamente identificados)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb14c3bb",
   "metadata": {},
   "source": [
    "### 5.4 Curva ROC y AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_test_cancer, y_pred_proba_cancer)\n",
    "roc_auc = roc_auc_score(y_test_cancer, y_pred_proba_cancer)\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "         label=f'SVM RBF (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "         label='Clasificador Aleatorio (AUC = 0.5)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
    "plt.title('Curva ROC - SVM para Detecci√≥n de C√°ncer')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ AUC = {roc_auc:.3f}\")\n",
    "print(\"   AUC > 0.95 indica excelente capacidad de discriminaci√≥n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8088b6b",
   "metadata": {},
   "source": [
    "### 5.5 Importancia de caracter√≠sticas (aproximada)\n",
    "\n",
    "SVM no proporciona importancia de caracter√≠sticas directamente, pero podemos aproximarla con los coeficientes del modelo lineal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d01f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar un SVM lineal para ver coeficientes\n",
    "svm_linear_cancer = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm_linear_cancer.fit(X_train_cancer_scaled, y_train_cancer)\n",
    "\n",
    "# Obtener coeficientes\n",
    "coef = pd.Series(\n",
    "    np.abs(svm_linear_cancer.coef_[0]), \n",
    "    index=X_cancer.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "# Visualizar top 10\n",
    "plt.figure(figsize=(10, 6))\n",
    "coef[:10].plot(kind='barh', color='steelblue')\n",
    "plt.xlabel('Importancia Absoluta (|coeficiente|)')\n",
    "plt.title('Top 10 Caracter√≠sticas M√°s Importantes (SVM Lineal)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìã Top 5 caracter√≠sticas m√°s influyentes:\")\n",
    "for i, (feature, importance) in enumerate(coef[:5].items(), 1):\n",
    "    print(f\"   {i}. {feature}: {importance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcea05d",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Comparaci√≥n de Kernels\n",
    "\n",
    "Comparemos el rendimiento de diferentes kernels en el dataset de c√°ncer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "results = []\n",
    "\n",
    "for kernel in kernels:\n",
    "    # Entrenar modelo\n",
    "    svm_k = SVC(kernel=kernel, C=1.0, random_state=42, gamma='scale')\n",
    "    svm_k.fit(X_train_cancer_scaled, y_train_cancer)\n",
    "    \n",
    "    # Predecir\n",
    "    y_pred_k = svm_k.predict(X_test_cancer_scaled)\n",
    "    \n",
    "    # Guardar resultados\n",
    "    accuracy = accuracy_score(y_test_cancer, y_pred_k)\n",
    "    results.append({\n",
    "        'Kernel': kernel,\n",
    "        'Accuracy': accuracy,\n",
    "        'N¬∞ Vectores de Soporte': len(svm_k.support_vectors_)\n",
    "    })\n",
    "    \n",
    "# Mostrar resultados\n",
    "results_df = pd.DataFrame(results).sort_values('Accuracy', ascending=False)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARACI√ìN DE KERNELS\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualizar\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "ax1.bar(results_df['Kernel'], results_df['Accuracy'], color='steelblue', alpha=0.7)\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Accuracy por Kernel')\n",
    "ax1.set_ylim([0.9, 1.0])\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(results_df['Accuracy']):\n",
    "    ax1.text(i, v + 0.005, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# N√∫mero de vectores de soporte\n",
    "ax2.bar(results_df['Kernel'], results_df['N¬∞ Vectores de Soporte'], color='coral', alpha=0.7)\n",
    "ax2.set_ylabel('N√∫mero de Vectores de Soporte')\n",
    "ax2.set_title('Complejidad del Modelo (Vectores de Soporte)')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(results_df['N¬∞ Vectores de Soporte']):\n",
    "    ax2.text(i, v + 5, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc1647",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Conclusiones\n",
    "\n",
    "### ‚úÖ Lo que hemos aprendido:\n",
    "\n",
    "1. **SVM Lineal:**\n",
    "   - Funciona perfectamente cuando los datos son linealmente separables\n",
    "   - Busca el hiperplano con el **margen m√°ximo**\n",
    "   - Solo depende de los **vectores de soporte**\n",
    "\n",
    "2. **Kernel Trick:**\n",
    "   - Soluci√≥n elegante para datos NO linealmente separables\n",
    "   - **Kernel RBF**: Transforma impl√≠citamente a un espacio de dimensi√≥n infinita\n",
    "   - **Gamma**: Controla la influencia local/global de cada punto\n",
    "\n",
    "3. **Par√°metro C:**\n",
    "   - Trade-off entre margen amplio y clasificaci√≥n correcta\n",
    "   - C alto ‚Üí Menos errores en training (riesgo overfitting)\n",
    "   - C bajo ‚Üí Margen m√°s amplio (mejor generalizaci√≥n)\n",
    "\n",
    "4. **Aplicaci√≥n Real:**\n",
    "   - SVM es excelente para datasets m√©dicos (alta precisi√≥n)\n",
    "   - GridSearchCV es esencial para encontrar hiperpar√°metros √≥ptimos\n",
    "   - **Siempre escalar los datos** antes de usar SVM\n",
    "\n",
    "### üéØ Ventajas de SVM:\n",
    "- ‚úÖ Muy efectivo en espacios de alta dimensi√≥n\n",
    "- ‚úÖ Eficiente en memoria (solo usa vectores de soporte)\n",
    "- ‚úÖ Vers√°til gracias a los diferentes kernels\n",
    "- ‚úÖ Funciona bien con datasets peque√±os/medianos\n",
    "\n",
    "### ‚ö†Ô∏è Desventajas de SVM:\n",
    "- ‚ùå Lento en datasets muy grandes (n > 10,000)\n",
    "- ‚ùå Requiere escalado de caracter√≠sticas\n",
    "- ‚ùå Dif√≠cil interpretabilidad (especialmente con kernels no lineales)\n",
    "- ‚ùå Sensible a la selecci√≥n de hiperpar√°metros\n",
    "\n",
    "### üìö Pr√≥ximos Pasos:\n",
    "- Experimentar con otros kernels (polynomial, sigmoid)\n",
    "- Probar SVM en problemas multiclase\n",
    "- Combinar SVM con t√©cnicas de reducci√≥n de dimensionalidad (PCA)\n",
    "- Comparar SVM con otros algoritmos (Random Forest, XGBoost)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
