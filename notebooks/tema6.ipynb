{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24bce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "\n",
    "# 1. Configuración de datos base\n",
    "# Utilizaremos el dataset Iris para ejemplos de clasificación multiclase\n",
    "# y el dataset Breast Cancer para ejemplos binarios.\n",
    "iris = datasets.load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X_cancer, y_cancer = cancer.data, cancer.target\n",
    "\n",
    "# División de datos (Entrenamiento y Prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42)\n",
    "\n",
    "def evaluate_model(model, X_t, y_t, name):\n",
    "    y_pred = model.predict(X_t)\n",
    "    acc = accuracy_score(y_t, y_pred)\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_t, y_pred))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef9b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 6.1. Árboles de Decisión\n",
    "# =================:=========================\n",
    "print(\"Ejecutando Árbol de Decisión...\")\n",
    "dt_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# Visualización del árbol\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(dt_clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
    "plt.title(\"Árbol de Decisión (Iris Dataset)\")\n",
    "plt.show()\n",
    "\n",
    "evaluate_model(dt_clf, X_test, y_test, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b028dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 6.2. SVM (Máquinas de Vectores de Soporte)\n",
    "# ==========================================\n",
    "print(\"Ejecutando SVM...\")\n",
    "# Usamos un kernel RBF (Radial Basis Function) que es el estándar\n",
    "svm_clf = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "evaluate_model(svm_clf, X_test, y_test, \"SVM (Kernel RBF)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65105987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 6.3. Naive Bayes\n",
    "# ==========================================\n",
    "print(\"Ejecutando Naive Bayes...\")\n",
    "# GaussianNB asume que las características siguen una distribución normal\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(X_train, y_train)\n",
    "\n",
    "evaluate_model(nb_clf, X_test, y_test, \"Naive Bayes (Gaussiano)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e327ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 6.4. Ensamblados: Bootstrapping, Bagging y Boosting\n",
    "# ==========================================\n",
    "\n",
    "# A. Bagging (Bootstrap Aggregating)\n",
    "# Entrena múltiples árboles en subconjuntos aleatorios de los datos (con reemplazo)\n",
    "print(\"Ejecutando Bagging...\")\n",
    "bagging_clf = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=50,\n",
    "    max_samples=0.8, # Bootstrapping de filas\n",
    "    random_state=42\n",
    ")\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "evaluate_model(bagging_clf, X_test, y_test, \"Bagging (50 Árboles)\")\n",
    "\n",
    "# B. Boosting (AdaBoost)\n",
    "# Entrena modelos secuencialmente, dando más peso a los errores del modelo anterior\n",
    "print(\"Ejecutando Boosting (AdaBoost)...\")\n",
    "boosting_clf = AdaBoostClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "boosting_clf.fit(X_train, y_train)\n",
    "evaluate_model(boosting_clf, X_test, y_test, \"AdaBoost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 6.5. Random Forest\n",
    "# ==========================================\n",
    "# Es un tipo especial de Bagging que también selecciona características al azar\n",
    "print(\"Ejecutando Random Forest...\")\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Importancia de las características\n",
    "importances = rf_clf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Importancia de Características - Random Forest')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [iris.feature_names[i] for i in indices])\n",
    "plt.xlabel('Importancia Relativa')\n",
    "plt.show()\n",
    "\n",
    "evaluate_model(rf_clf, X_test, y_test, \"Random Forest\")\n",
    "\n",
    "print(\"Notebook finalizado correctamente.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
